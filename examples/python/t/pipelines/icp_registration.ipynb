{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# monkey patches visualization and provides helpers to load geometries\n",
    "sys.path.append('..')\n",
    "import open3d_tutorial as o3dtut\n",
    "# change to True if you want to interact with the visualization windows\n",
    "o3dtut.interactive = not \"CI\" in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper visualization function\n",
    "The function below visualizes a target point cloud and a source point cloud transformed with an alignment transformation. The target point cloud and the source point cloud are painted with cyan and yellow colors respectively. The more and tighter the two point clouds overlap with each other, the better the alignment result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = source.clone()\n",
    "    target_temp = target.clone()\n",
    "\n",
    "    source_temp.transform(transformation.to(o3d.core.Dtype.Float32))\n",
    "    o3d.visualization.draw([source_temp, target_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ICP registration\n",
    "This tutorial demonstrates the ICP (Iterative Closest Point) registration algorithm. It has been a mainstay of geometric registration in both research and industry for many years. The input are two point clouds and an initial transformation that roughly aligns the source point cloud to the target point cloud. The output is a refined transformation that tightly aligns the two point clouds. A helper function `draw_registration_result` visualizes the alignment during the registration process. In this tutorial, we show two ICP variants, the point-to-point ICP and the point-to-plane ICP [\\[Rusinkiewicz2001\\]](../reference.html#rusinkiewicz2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ICP Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the ICP algorithm iterates over two steps:\n",
    "\n",
    "1. Find correspondence set $\\mathcal{K}=\\{(\\mathbf{p}, \\mathbf{q})\\}$ from target point cloud $\\mathbf{P}$, and source point cloud $\\mathbf{Q}$ transformed with current transformation matrix $\\mathbf{T}$.\n",
    "2. Update the transformation $\\mathbf{T}$ by minimizing an objective function $E(\\mathbf{T})$ defined over the correspondence set $\\mathcal{K}$.\n",
    "\n",
    "Different variants of ICP use different objective functions $E(\\mathbf{T})$ [\\[BeslAndMcKay1992\\]](../reference.html#beslandmckay1992) [\\[ChenAndMedioni1992\\]](../reference.html#chenandmedioni1992) [\\[Park2017\\]](../reference.html#park2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ICP API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Point-to-point ICP\n",
    "\n",
    "We first show a point-to-point ICP algorithm [\\[BeslAndMcKay1992\\]](../reference.html#beslandmckay1992) using the objective\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\mathbf{T}) = \\sum_{(\\mathbf{p},\\mathbf{q})\\in\\mathcal{K}}\\|\\mathbf{p} - \\mathbf{T}\\mathbf{q}\\|^{2}\n",
    "\\end{equation}\n",
    "\n",
    "The class `TransformationEstimationPointToPoint` provides functions to compute the residuals and Jacobian matrices of the point-to-point ICP objective. The function `icp` and `multi_scale_icp` takes it as a parameter and runs point-to-point ICP to obtain the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Point-to-plane ICP\n",
    "The point-to-plane ICP algorithm [\\[ChenAndMedioni1992\\]](../reference.html#chenandmedioni1992) uses a different objective function\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\mathbf{T}) = \\sum_{(\\mathbf{p},\\mathbf{q})\\in\\mathcal{K}}\\big((\\mathbf{p} - \\mathbf{T}\\mathbf{q})\\cdot\\mathbf{n}_{\\mathbf{p}}\\big)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{n}_{\\mathbf{p}}$ is the normal of point $\\mathbf{p}$. [\\[Rusinkiewicz2001\\]](../reference.html#rusinkiewicz2001) has shown that the point-to-plane ICP algorithm has a faster convergence speed than the point-to-point ICP algorithm.\n",
    "\n",
    "The class `TransformationEstimationPointToPlane` provides functions to compute the residuals and Jacobian matrices of the point-to-plane ICP objective. The function `icp` and `multi_scale_icp` takes it as a parameter and runs point-to-plane ICP to obtain the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colored point cloud registration\n",
    "\n",
    "Following [\\[Park2017\\]](../reference.html#park2017), it runs ICP iterations with a joint optimization objective\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\mathbf{T}) = (1-\\delta)E_{C}(\\mathbf{T}) + \\delta E_{G}(\\mathbf{T})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{T}$ is the transformation matrix to be estimated. $E_{C}$ and $E_{G}$ are the photometric and geometric terms, respectively. $\\delta\\in[0,1]$ is a weight parameter that has been determined empirically.\n",
    "\n",
    "The geometric term $E_{G}$ is the same as the [Point-to-plane ICP](../pipelines/icp_registration.ipynb#Point-to-plane-ICP) objective\n",
    "\n",
    "\\begin{equation}\n",
    "E_{G}(\\mathbf{T}) = \\sum_{(\\mathbf{p},\\mathbf{q})\\in\\mathcal{K}}\\big((\\mathbf{p} - \\mathbf{T}\\mathbf{q})\\cdot\\mathbf{n}_{\\mathbf{p}}\\big)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{K}$ is the correspondence set in the current iteration. $\\mathbf{n}_{\\mathbf{p}}$ is the normal of point $\\mathbf{p}$.\n",
    "\n",
    "The color term $E_{C}$ measures the difference between the color of point $\\mathbf{q}$ (denoted as $C(\\mathbf{q})$) and the color of its projection on the tangent plane of $\\mathbf{p}$.\n",
    "\n",
    "\\begin{equation}\n",
    "E_{C}(\\mathbf{T}) = \\sum_{(\\mathbf{p},\\mathbf{q})\\in\\mathcal{K}}\\big(C_{\\mathbf{p}}(\\mathbf{f}(\\mathbf{T}\\mathbf{q})) - C(\\mathbf{q})\\big)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "where $C_{\\mathbf{p}}(\\cdot)$ is a precomputed function continuously defined on the tangent plane of $\\mathbf{p}$. Function$\\mathbf{f}(\\cdot)$ projects a 3D point to the tangent plane. For more details, refer to [\\[Park2017\\]](../reference.html#park2017).\n",
    "\n",
    "To further improve efficiency, [\\[Park2017\\]](../reference.html#park2017) proposes a multi-scale registration scheme. \n",
    "\n",
    "The class `TransformationEstimationForColoredICP` provides functions to compute the residuals and Jacobian matrices of the joint optimization objective. The function `icp` and `multi_scale_icp` takes it as a parameter and runs colored ICP to obtain the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input\n",
    "The code below reads a source point cloud and a target point cloud from two files. A rough transformation is given.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note:** \n",
    "\n",
    "The initial alignment is usually obtained by a global registration algorithm. See [Global registration](../pipelines/global_registration.rst) for examples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = o3d.t.io.read_point_cloud(\"../../test_data/ColoredICP/frag_115.ply\")\n",
    "target = o3d.t.io.read_point_cloud(\"../../test_data/ColoredICP/frag_116.ply\")\n",
    "\n",
    "# For Colored-ICP `colors` attribute must be of same dtype as `positions` and `normals` attribute.\n",
    "source.point[\"colors\"] = source.point[\"colors\"].to(\n",
    "    o3d.core.Dtype.Float32) / 255.0\n",
    "target.point[\"colors\"] = target.point[\"colors\"].to(\n",
    "    o3d.core.Dtype.Float32) / 255.0\n",
    "\n",
    "trans_init = o3d.core.Tensor.eye(4, o3d.core.Dtype.Float32)\n",
    "\n",
    "draw_registration_result(source, target, trans_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python API » open3d.t » open3d.t.pipelines » open3d.t.pipelines.registration » open3d.t.pipelines.registration.multi_scale_icp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input PointClouds between which the `Transformation` is to be estimated. [open3d.t.PointCloud]\n",
    "- Source Tensor PointCloud. [Float32 or Float64 dtypes are supported].\n",
    "- Target Tensor PointCloud. [Float32 or Float64 dtypes are supported]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max correspondence Distances\n",
    "- This is the radius of distance from each point in source point-cloud in which the neighbour search will try to find an corresponding point in the target point-cloud.\n",
    "- It is a `double` for `ICP`, and `utility.DoubleVector` for `Multi-Scale-ICP`.\n",
    "- One may typically keep this parameter between 1.0x - 3.0x `voxel-size` for each scale.\n",
    "- This parameter is most important for performance tuning, as higher radius will take larget time (as the neighbour search will be performed over a larger radius)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inital Transform from Source to Target [open3d.core.Tensor]\n",
    "- Initial estimate for transfromation from source to target.\n",
    "- Transformation matrix Tensor of shape [4, 4] of type `Float64` on `CPU:0` device\n",
    "- The initial alignment is usually obtained by a global registration algorithm. See [Global registration](../pipelines/global_registration.rst) for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation Method \n",
    "- This sets the ICP method to compute the transformation between two point-clouds given the correspondences.\n",
    "\n",
    "Options:\n",
    "\n",
    "- **o3d.t.pipelines.registration.TransformationEstimationPointToPoint()**\n",
    "    - Point to Point ICP.\n",
    "- **o3d.t.pipelines.registration.TransformationEstimationPointToPlane(robust_kernel)**\n",
    "    - Point to Plane ICP.\n",
    "    - Requires `target point-cloud` to have `normals` attribute (of same dtype as `position` attribute).\n",
    "- **o3d.t.pipelines.registration.TransformationEstimationForColoredICP(robust_kernel, lambda)**\n",
    "    - Colored ICP.\n",
    "    - Requires `target` point-cloud to have `normals` attribute (of same dtype as `position` attribute).\n",
    "    - Requires `source` and `target` point-clouds to have `colors` attribute (of same dtype as `position` attribute).\n",
    "- **o3d.t.pipelines.registration.TransformationEstimationForGeneralizedICP(robust_kernel, epsilon)** [To be added].\n",
    "    - Generalized ICP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation Method supports `Robust Kernels`. \n",
    "- Robust kernels are used for outlier rejection.\n",
    "\n",
    "`robust_kernel = o3d.t.pipelines.registration.robust_kernel.RobustKernel(method, scale, shape)`\n",
    "Method options\n",
    "- robust_kernel.RobustKernelMethod.L2Loss\n",
    "- robust_kernel.RobustKernelMethod.L1Loss\n",
    "- robust_kernel.RobustKernelMethod.HuberLoss\n",
    "- robust_kernel.RobustKernelMethod.CauchyLoss\n",
    "- robust_kernel.RobustKernelMethod.GMLoss\n",
    "- robust_kernel.RobustKernelMethod.TurkeyLoss\n",
    "- robust_kernel.RobustKernelMethod.GeneralizedLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICP Convergence Criteria [relative rmse, relative fitness, max iterations]\n",
    "- This sets the condition for termination or when the scale iterations can be considered to be converged. \n",
    "- If the relative (of change in value from last iteration) rmse and fitness is equal or less than the specified value, the iterations for that scale will be considered as converged / completed.\n",
    "- For `Multi-Scale ICP` it is a `list` of `ICPConvergenceCriteria`, for each scale of ICP, to provide more fine control over performance.\n",
    "- One may keep the initial values of `relative_fitness` and `relative_rmse` low as we just want to get an estimate transformation, and high for later iterations to fine-tune.\n",
    "- Iterations on higher-resolution is more costly (takes more time), so we want to do less iterations on higher resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Sizes [open3d.utility.DoubleVector]\n",
    "- It is the voxel size (analogous to resolution in case of image), for each scale of mult-scale ICP.\n",
    "- We want to perform initial iterations on a coarse point-cloud (low-resolution or small voxel size) (as it is more time-efficient, and avoids local-minima), and then move to dense point-cloud (high-resolution or small voxel size. Therefore the voxel sizes must be strictly decreasing order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if o3d.__DEVICE_API__ == 'cuda':\n",
    "    import open3d.cuda.pybind.t.pipelines.registration as treg\n",
    "else:\n",
    "    import open3d.cpu.pybind.t.pipelines.registration as treg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = treg.ICPConvergenceCriteria(relative_fitness=0.000001,\n",
    "                                       relative_rmse=0.000001,\n",
    "                                       max_iteration=50)\n",
    "\n",
    "max_correspondence_distances = 0.03\n",
    "\n",
    "init_source_to_target = trans_init\n",
    "\n",
    "estimation = treg.TransformationEstimationForColoredICP(\n",
    "    treg.robust_kernel.RobustKernel(\n",
    "        treg.robust_kernel.RobustKernelMethod.L2Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "source_icp = source.voxel_down_sample(0.0125)\n",
    "target_icp = target.voxel_down_sample(0.0125)\n",
    "\n",
    "registration_result = treg.icp(source_icp, target_icp,\n",
    "                               max_correspondence_distances,\n",
    "                               init_source_to_target, estimation, criteria)\n",
    "\n",
    "icp_time = time.time() - s\n",
    "print(\"Time taken by ICP: \", icp_time)\n",
    "print(\"Inlier Fitness: \", registration_result.fitness)\n",
    "print(\"Inlier RMSE: \", registration_result.inlier_rmse)\n",
    "\n",
    "draw_registration_result(source, target, registration_result.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale ICP Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_sizes = o3d.utility.DoubleVector([0.05, 0.025, 0.0125])\n",
    "\n",
    "criteria_list = [\n",
    "    treg.ICPConvergenceCriteria(relative_fitness=0.0001,\n",
    "                                relative_rmse=0.0001,\n",
    "                                max_iteration=20),\n",
    "    treg.ICPConvergenceCriteria(0.00001, 0.00001, 15),\n",
    "    treg.ICPConvergenceCriteria(0.000001, 0.000001, 10)\n",
    "]\n",
    "\n",
    "max_correspondence_distances = o3d.utility.DoubleVector([0.7, 0.07, 0.03])\n",
    "\n",
    "init_source_to_target = trans_init\n",
    "\n",
    "estimation = treg.TransformationEstimationForColoredICP(\n",
    "    treg.robust_kernel.RobustKernel(\n",
    "        treg.robust_kernel.RobustKernelMethod.L2Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Verbosity to Debug, helps in fine-tuning the performance.\n",
    "# o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "registration_result = treg.multi_scale_icp(source, target, voxel_sizes,\n",
    "                                           criteria_list,\n",
    "                                           max_correspondence_distances,\n",
    "                                           init_source_to_target, estimation)\n",
    "\n",
    "ms_icp_time = time.time() - s\n",
    "print(\"Time taken by Multi-Scale ICP: \", ms_icp_time)\n",
    "print(\"Inlier Fitness: \", registration_result.fitness)\n",
    "print(\"Inlier RMSE: \", registration_result.inlier_rmse)\n",
    "\n",
    "draw_registration_result(source, target, registration_result.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale ICP on CUDA device Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm runs on the same device as the source and target point-cloud.\n",
    "source = source.cuda(0)\n",
    "target = target.cuda(0)\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "registration_result = treg.multi_scale_icp(source, target, voxel_sizes,\n",
    "                                           criteria_list,\n",
    "                                           max_correspondence_distances,\n",
    "                                           init_source_to_target, estimation)\n",
    "\n",
    "ms_icp_time = time.time() - s\n",
    "print(\"Time taken by Multi-Scale ICP: \", ms_icp_time)\n",
    "print(\"Inlier Fitness: \", registration_result.fitness)\n",
    "print(\"Inlier RMSE: \", registration_result.inlier_rmse)\n",
    "\n",
    "draw_registration_result(source.cpu(), target.cpu(),\n",
    "                         registration_result.transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_matrix = treg.get_information_matrix(\n",
    "    source, target, max_correspondence_distances[2],\n",
    "    registration_result.transformation)\n",
    "\n",
    "print(information_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
